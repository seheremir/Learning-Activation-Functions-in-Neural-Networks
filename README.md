# Learning Activation Functions in Neural Networks

This repository presents an experimental study on **learning and adapting activation functions in neural networks**, instead of relying on fixed, manually chosen activations such as ReLU, Tanh, or Sigmoid.

The project investigates whether neural networks can **learn their own activation behavior** during training and how this affects performance, stability, and generalization across different experimental settings.

---

## Paper (Detailed Report)

A full academic-style report describing the motivation, methodology, experiments, and results of this project is available as a PDF:

ðŸ‘‰ **[Read the full paper (PDF)](Learning-Activation-Functions-in-Neural-Networks.pdf)**

> The PDF opens directly in GitHubâ€™s built-in viewer and can be downloaded if needed.

For a deeper theoretical discussion, experimental design details, and comprehensive visual analysis, please refer to the paper above.

---

## Notebook

The main experimental workflow is implemented in the following Jupyter notebook:

- `aktivasyon_fonk_tahmini.ipynb`

The notebook is designed to run on **Google Colab** and contains:
- Model definitions
- Adaptive activation mechanisms
- Training and evaluation loops
- Automatic result logging and visualization

---

## Experiments and Results

Key outputs generated by the experiments are stored under:

